{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for dataLoader construction, training, and testing is largely referred from the complementary material from PyTorch course on Udacity. \n",
    "\n",
    "Link to the course: https://www.udacity.com/course/deep-learning-pytorch--ud188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0 1 2 3]\n[0 1 2 3]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "print(np.unique(y_train_valid))\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training/Valid data shape: (2115, 22, 1000)\nTest data shape: (443, 22, 1000)\nTraining/Valid target shape: (2115,)\nTest target shape: (443,)\nPerson train/valid shape: (2115, 1)\nPerson test shape: (443, 1)\n"
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use All Data (run this or either of the following 2, can't run them altogether)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "X_train_valid = torch.from_numpy(X_train_valid).float()\n",
    "y_train_valid = torch.from_numpy(y_train_valid).long()\n",
    "train_data = data.TensorDataset(X_train_valid, y_train_valid)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "test_data = data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a portion of the data in time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "time_upper_bound = 840\n",
    "\n",
    "X_train_valid = torch.from_numpy(X_train_valid[:,:,:time_upper_bound]).float()\n",
    "y_train_valid = torch.from_numpy(y_train_valid).long()\n",
    "train_data = data.TensorDataset(X_train_valid, y_train_valid)\n",
    "\n",
    "X_test = torch.from_numpy(X_test[:,:,:time_upper_bound]).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "test_data = data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Specific Subject's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n"
    }
   ],
   "source": [
    "person_train_valid = person_train_valid.squeeze()\n",
    "person_test = person_test.squeeze()\n",
    "print(np.unique(person_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "subject_id = 1.\n",
    "\n",
    "X_train_valid = torch.from_numpy(X_train_valid[person_train_valid==subject_id]).float()\n",
    "y_train_valid = torch.from_numpy(y_train_valid[person_train_valid==subject_id]).long()\n",
    "train_data = data.TensorDataset(X_train_valid, y_train_valid)\n",
    "\n",
    "X_test = torch.from_numpy(X_test[person_test==subject_id]).float()\n",
    "y_test = torch.from_numpy(y_test[person_test==subject_id]).long()\n",
    "test_data = data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "batch_size = 128\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cuda:0\nTraining on GPU ...\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
    "print(device)\n",
    "if not train_on_gpu:\n",
    "    print('Training on CPU ...')\n",
    "else:\n",
    "    print('Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(22, 64, kernel_size=22, stride=2),\n",
    "            nn.BatchNorm1d(64, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=12, stride=2),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(64, 192, kernel_size=12),\n",
    "            nn.BatchNorm1d(192, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(192, 384, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm1d(384, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(384, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm1d(256, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(256, 256, kernel_size=4),\n",
    "            nn.BatchNorm1d(256, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=1),\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm1d(256 * 21),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256 * 21),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256 * 21, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = AlexNet()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./tensorboard/cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.0005, lr=1e-3)\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "g Loss: 0.395867 \tValidation Loss: 1.116173\nEpoch: 1690 \tTraining Loss: 0.386626 \tValidation Loss: 1.119320\nEpoch: 1691 \tTraining Loss: 0.415857 \tValidation Loss: 0.973674\nEpoch: 1692 \tTraining Loss: 0.396505 \tValidation Loss: 1.091113\nEpoch: 1693 \tTraining Loss: 0.426841 \tValidation Loss: 1.186971\nEpoch: 1694 \tTraining Loss: 0.416552 \tValidation Loss: 1.020351\nEpoch: 1695 \tTraining Loss: 0.410623 \tValidation Loss: 1.041588\nEpoch: 1696 \tTraining Loss: 0.400647 \tValidation Loss: 1.005105\nEpoch: 1697 \tTraining Loss: 0.363640 \tValidation Loss: 1.020974\nEpoch: 1698 \tTraining Loss: 0.406850 \tValidation Loss: 0.974718\nEpoch: 1699 \tTraining Loss: 0.381044 \tValidation Loss: 0.958370\nEpoch: 1700 \tTraining Loss: 0.385796 \tValidation Loss: 0.997682\nEpoch: 1701 \tTraining Loss: 0.410847 \tValidation Loss: 1.045420\nEpoch: 1702 \tTraining Loss: 0.398542 \tValidation Loss: 0.956403\nEpoch: 1703 \tTraining Loss: 0.378763 \tValidation Loss: 0.964045\nEpoch: 1704 \tTraining Loss: 0.383844 \tValidation Loss: 1.032094\nEpoch: 1705 \tTraining Loss: 0.380651 \tValidation Loss: 1.003156\nEpoch: 1706 \tTraining Loss: 0.381241 \tValidation Loss: 0.984921\nEpoch: 1707 \tTraining Loss: 0.393394 \tValidation Loss: 1.090621\nEpoch: 1708 \tTraining Loss: 0.402357 \tValidation Loss: 1.198382\nEpoch: 1709 \tTraining Loss: 0.422003 \tValidation Loss: 0.940975\nEpoch: 1710 \tTraining Loss: 0.382497 \tValidation Loss: 0.938846\nEpoch: 1711 \tTraining Loss: 0.380619 \tValidation Loss: 1.028647\nEpoch: 1712 \tTraining Loss: 0.403852 \tValidation Loss: 0.961553\nEpoch: 1713 \tTraining Loss: 0.404970 \tValidation Loss: 0.984054\nEpoch: 1714 \tTraining Loss: 0.389197 \tValidation Loss: 1.060908\nEpoch: 1715 \tTraining Loss: 0.403331 \tValidation Loss: 1.032152\nEpoch: 1716 \tTraining Loss: 0.395497 \tValidation Loss: 0.991251\nEpoch: 1717 \tTraining Loss: 0.388310 \tValidation Loss: 1.044903\nEpoch: 1718 \tTraining Loss: 0.387337 \tValidation Loss: 0.964575\nEpoch: 1719 \tTraining Loss: 0.382311 \tValidation Loss: 1.174439\nEpoch: 1720 \tTraining Loss: 0.404569 \tValidation Loss: 0.908845\nEpoch: 1721 \tTraining Loss: 0.412735 \tValidation Loss: 0.952390\nEpoch: 1722 \tTraining Loss: 0.401152 \tValidation Loss: 0.972178\nEpoch: 1723 \tTraining Loss: 0.380387 \tValidation Loss: 1.115368\nEpoch: 1724 \tTraining Loss: 0.398300 \tValidation Loss: 0.929483\nEpoch: 1725 \tTraining Loss: 0.386257 \tValidation Loss: 0.920351\nEpoch: 1726 \tTraining Loss: 0.393543 \tValidation Loss: 1.149669\nEpoch: 1727 \tTraining Loss: 0.402162 \tValidation Loss: 1.222998\nEpoch: 1728 \tTraining Loss: 0.412984 \tValidation Loss: 1.170814\nEpoch: 1729 \tTraining Loss: 0.382044 \tValidation Loss: 1.069130\nEpoch: 1730 \tTraining Loss: 0.375133 \tValidation Loss: 0.997764\nEpoch: 1731 \tTraining Loss: 0.399104 \tValidation Loss: 0.910156\nEpoch: 1732 \tTraining Loss: 0.423622 \tValidation Loss: 1.106156\nEpoch: 1733 \tTraining Loss: 0.428950 \tValidation Loss: 1.126096\nEpoch: 1734 \tTraining Loss: 0.391526 \tValidation Loss: 1.037263\nEpoch: 1735 \tTraining Loss: 0.356820 \tValidation Loss: 1.273444\nEpoch: 1736 \tTraining Loss: 0.378093 \tValidation Loss: 1.046521\nEpoch: 1737 \tTraining Loss: 0.403774 \tValidation Loss: 1.097527\nEpoch: 1738 \tTraining Loss: 0.406957 \tValidation Loss: 1.040922\nEpoch: 1739 \tTraining Loss: 0.375644 \tValidation Loss: 1.135658\nEpoch: 1740 \tTraining Loss: 0.396128 \tValidation Loss: 1.236997\nEpoch: 1741 \tTraining Loss: 0.382135 \tValidation Loss: 1.001317\nEpoch: 1742 \tTraining Loss: 0.402162 \tValidation Loss: 1.189750\nEpoch: 1743 \tTraining Loss: 0.385484 \tValidation Loss: 1.006262\nEpoch: 1744 \tTraining Loss: 0.393384 \tValidation Loss: 1.174108\nEpoch: 1745 \tTraining Loss: 0.418686 \tValidation Loss: 0.975340\nEpoch: 1746 \tTraining Loss: 0.395299 \tValidation Loss: 1.070256\nEpoch: 1747 \tTraining Loss: 0.432115 \tValidation Loss: 0.993905\nEpoch: 1748 \tTraining Loss: 0.392908 \tValidation Loss: 1.143211\nEpoch: 1749 \tTraining Loss: 0.477546 \tValidation Loss: 1.104563\nEpoch: 1750 \tTraining Loss: 0.422103 \tValidation Loss: 0.991364\nEpoch: 1751 \tTraining Loss: 0.391118 \tValidation Loss: 1.033120\nEpoch: 1752 \tTraining Loss: 0.386458 \tValidation Loss: 1.018741\nEpoch: 1753 \tTraining Loss: 0.382522 \tValidation Loss: 1.006357\nEpoch: 1754 \tTraining Loss: 0.410172 \tValidation Loss: 1.088498\nEpoch: 1755 \tTraining Loss: 0.396581 \tValidation Loss: 1.042126\nEpoch: 1756 \tTraining Loss: 0.379605 \tValidation Loss: 1.140611\nEpoch: 1757 \tTraining Loss: 0.378339 \tValidation Loss: 1.052004\nEpoch: 1758 \tTraining Loss: 0.383030 \tValidation Loss: 1.072059\nEpoch: 1759 \tTraining Loss: 0.396532 \tValidation Loss: 1.088656\nEpoch: 1760 \tTraining Loss: 0.396592 \tValidation Loss: 1.021467\nEpoch: 1761 \tTraining Loss: 0.411904 \tValidation Loss: 1.206696\nEpoch: 1762 \tTraining Loss: 0.411063 \tValidation Loss: 1.118771\nEpoch: 1763 \tTraining Loss: 0.401539 \tValidation Loss: 0.998274\nEpoch: 1764 \tTraining Loss: 0.393797 \tValidation Loss: 1.083693\nEpoch: 1765 \tTraining Loss: 0.394317 \tValidation Loss: 1.004652\nEpoch: 1766 \tTraining Loss: 0.386107 \tValidation Loss: 1.006199\nEpoch: 1767 \tTraining Loss: 0.365276 \tValidation Loss: 1.065859\nEpoch: 1768 \tTraining Loss: 0.403234 \tValidation Loss: 1.101670\nEpoch: 1769 \tTraining Loss: 0.412552 \tValidation Loss: 0.926235\nEpoch: 1770 \tTraining Loss: 0.394665 \tValidation Loss: 1.190828\nEpoch: 1771 \tTraining Loss: 0.382373 \tValidation Loss: 1.026448\nEpoch: 1772 \tTraining Loss: 0.384332 \tValidation Loss: 1.000738\nEpoch: 1773 \tTraining Loss: 0.377471 \tValidation Loss: 1.114914\nEpoch: 1774 \tTraining Loss: 0.431545 \tValidation Loss: 1.097944\nEpoch: 1775 \tTraining Loss: 0.413078 \tValidation Loss: 0.921403\nEpoch: 1776 \tTraining Loss: 0.398987 \tValidation Loss: 1.098672\nEpoch: 1777 \tTraining Loss: 0.407383 \tValidation Loss: 0.966643\nEpoch: 1778 \tTraining Loss: 0.419694 \tValidation Loss: 1.170224\nEpoch: 1779 \tTraining Loss: 0.376848 \tValidation Loss: 0.973395\nEpoch: 1780 \tTraining Loss: 0.421279 \tValidation Loss: 1.076252\nEpoch: 1781 \tTraining Loss: 0.395651 \tValidation Loss: 1.084105\nEpoch: 1782 \tTraining Loss: 0.421795 \tValidation Loss: 0.979815\nEpoch: 1783 \tTraining Loss: 0.401659 \tValidation Loss: 0.995528\nEpoch: 1784 \tTraining Loss: 0.387604 \tValidation Loss: 1.211093\nEpoch: 1785 \tTraining Loss: 0.410329 \tValidation Loss: 0.938028\nEpoch: 1786 \tTraining Loss: 0.429665 \tValidation Loss: 1.096604\nEpoch: 1787 \tTraining Loss: 0.393122 \tValidation Loss: 1.010614\nEpoch: 1788 \tTraining Loss: 0.438953 \tValidation Loss: 1.089407\nEpoch: 1789 \tTraining Loss: 0.378443 \tValidation Loss: 1.131273\nEpoch: 1790 \tTraining Loss: 0.375153 \tValidation Loss: 0.981979\nEpoch: 1791 \tTraining Loss: 0.416641 \tValidation Loss: 1.261865\nEpoch: 1792 \tTraining Loss: 0.403774 \tValidation Loss: 1.026152\nEpoch: 1793 \tTraining Loss: 0.383929 \tValidation Loss: 1.038633\nEpoch: 1794 \tTraining Loss: 0.386564 \tValidation Loss: 1.008267\nEpoch: 1795 \tTraining Loss: 0.352074 \tValidation Loss: 1.070056\nEpoch: 1796 \tTraining Loss: 0.378960 \tValidation Loss: 1.052979\nEpoch: 1797 \tTraining Loss: 0.362355 \tValidation Loss: 1.048494\nEpoch: 1798 \tTraining Loss: 0.380910 \tValidation Loss: 1.161533\nEpoch: 1799 \tTraining Loss: 0.398112 \tValidation Loss: 0.977212\nEpoch: 1800 \tTraining Loss: 0.382128 \tValidation Loss: 1.026076\nEpoch: 1801 \tTraining Loss: 0.402082 \tValidation Loss: 1.129294\nEpoch: 1802 \tTraining Loss: 0.374085 \tValidation Loss: 0.948533\nEpoch: 1803 \tTraining Loss: 0.431453 \tValidation Loss: 0.944183\nEpoch: 1804 \tTraining Loss: 0.375754 \tValidation Loss: 1.050178\nEpoch: 1805 \tTraining Loss: 0.365842 \tValidation Loss: 0.951526\nEpoch: 1806 \tTraining Loss: 0.386590 \tValidation Loss: 1.143246\nEpoch: 1807 \tTraining Loss: 0.342558 \tValidation Loss: 1.088533\nEpoch: 1808 \tTraining Loss: 0.394646 \tValidation Loss: 1.018437\nEpoch: 1809 \tTraining Loss: 0.423509 \tValidation Loss: 1.023642\nEpoch: 1810 \tTraining Loss: 0.388891 \tValidation Loss: 1.217783\nEpoch: 1811 \tTraining Loss: 0.389548 \tValidation Loss: 1.041093\nEpoch: 1812 \tTraining Loss: 0.411131 \tValidation Loss: 0.958559\nEpoch: 1813 \tTraining Loss: 0.394912 \tValidation Loss: 1.128071\nEpoch: 1814 \tTraining Loss: 0.405194 \tValidation Loss: 0.945284\nEpoch: 1815 \tTraining Loss: 0.394121 \tValidation Loss: 1.023619\nEpoch: 1816 \tTraining Loss: 0.402610 \tValidation Loss: 1.013664\nEpoch: 1817 \tTraining Loss: 0.388079 \tValidation Loss: 0.966669\nEpoch: 1818 \tTraining Loss: 0.399093 \tValidation Loss: 0.959792\nEpoch: 1819 \tTraining Loss: 0.371794 \tValidation Loss: 0.988962\nEpoch: 1820 \tTraining Loss: 0.379817 \tValidation Loss: 1.059886\nEpoch: 1821 \tTraining Loss: 0.377229 \tValidation Loss: 1.094156\nEpoch: 1822 \tTraining Loss: 0.369269 \tValidation Loss: 1.078597\nEpoch: 1823 \tTraining Loss: 0.377203 \tValidation Loss: 1.124256\nEpoch: 1824 \tTraining Loss: 0.398930 \tValidation Loss: 1.077903\nEpoch: 1825 \tTraining Loss: 0.440101 \tValidation Loss: 1.109930\nEpoch: 1826 \tTraining Loss: 0.413388 \tValidation Loss: 1.111092\nEpoch: 1827 \tTraining Loss: 0.411000 \tValidation Loss: 1.124904\nEpoch: 1828 \tTraining Loss: 0.399518 \tValidation Loss: 1.058595\nEpoch: 1829 \tTraining Loss: 0.383749 \tValidation Loss: 1.073741\nEpoch: 1830 \tTraining Loss: 0.379988 \tValidation Loss: 1.059812\nEpoch: 1831 \tTraining Loss: 0.392606 \tValidation Loss: 1.175865\nEpoch: 1832 \tTraining Loss: 0.368439 \tValidation Loss: 0.984902\nEpoch: 1833 \tTraining Loss: 0.436294 \tValidation Loss: 0.979828\nEpoch: 1834 \tTraining Loss: 0.397206 \tValidation Loss: 1.044924\nEpoch: 1835 \tTraining Loss: 0.397475 \tValidation Loss: 0.998513\nEpoch: 1836 \tTraining Loss: 0.383991 \tValidation Loss: 1.146991\nEpoch: 1837 \tTraining Loss: 0.389991 \tValidation Loss: 1.110676\nEpoch: 1838 \tTraining Loss: 0.407062 \tValidation Loss: 1.132492\nEpoch: 1839 \tTraining Loss: 0.366922 \tValidation Loss: 1.115494\nEpoch: 1840 \tTraining Loss: 0.392478 \tValidation Loss: 1.112817\nEpoch: 1841 \tTraining Loss: 0.425890 \tValidation Loss: 1.093118\nEpoch: 1842 \tTraining Loss: 0.407164 \tValidation Loss: 1.049962\nEpoch: 1843 \tTraining Loss: 0.348621 \tValidation Loss: 1.110348\nEpoch: 1844 \tTraining Loss: 0.412247 \tValidation Loss: 1.042642\nEpoch: 1845 \tTraining Loss: 0.393233 \tValidation Loss: 1.072175\nEpoch: 1846 \tTraining Loss: 0.384578 \tValidation Loss: 0.913252\nEpoch: 1847 \tTraining Loss: 0.406009 \tValidation Loss: 1.199550\nEpoch: 1848 \tTraining Loss: 0.352844 \tValidation Loss: 1.136172\nEpoch: 1849 \tTraining Loss: 0.434681 \tValidation Loss: 0.978969\nEpoch: 1850 \tTraining Loss: 0.389739 \tValidation Loss: 1.051153\nEpoch: 1851 \tTraining Loss: 0.382615 \tValidation Loss: 1.091649\nEpoch: 1852 \tTraining Loss: 0.395416 \tValidation Loss: 1.178787\nEpoch: 1853 \tTraining Loss: 0.407739 \tValidation Loss: 0.964673\nEpoch: 1854 \tTraining Loss: 0.402114 \tValidation Loss: 0.932388\nEpoch: 1855 \tTraining Loss: 0.413162 \tValidation Loss: 1.057160\nEpoch: 1856 \tTraining Loss: 0.404040 \tValidation Loss: 0.994040\nEpoch: 1857 \tTraining Loss: 0.409142 \tValidation Loss: 1.075997\nEpoch: 1858 \tTraining Loss: 0.374916 \tValidation Loss: 1.195859\nEpoch: 1859 \tTraining Loss: 0.350122 \tValidation Loss: 1.049104\nEpoch: 1860 \tTraining Loss: 0.416882 \tValidation Loss: 1.009097\nEpoch: 1861 \tTraining Loss: 0.405079 \tValidation Loss: 1.033006\nEpoch: 1862 \tTraining Loss: 0.401071 \tValidation Loss: 1.085559\nEpoch: 1863 \tTraining Loss: 0.382930 \tValidation Loss: 1.218010\nEpoch: 1864 \tTraining Loss: 0.388389 \tValidation Loss: 1.006875\nEpoch: 1865 \tTraining Loss: 0.365320 \tValidation Loss: 1.006885\nEpoch: 1866 \tTraining Loss: 0.346901 \tValidation Loss: 0.976917\nEpoch: 1867 \tTraining Loss: 0.404063 \tValidation Loss: 1.134839\nEpoch: 1868 \tTraining Loss: 0.396287 \tValidation Loss: 0.953307\nEpoch: 1869 \tTraining Loss: 0.405085 \tValidation Loss: 0.979283\nEpoch: 1870 \tTraining Loss: 0.395972 \tValidation Loss: 0.958372\nEpoch: 1871 \tTraining Loss: 0.392858 \tValidation Loss: 1.020421\nEpoch: 1872 \tTraining Loss: 0.396030 \tValidation Loss: 1.303996\nEpoch: 1873 \tTraining Loss: 0.401313 \tValidation Loss: 1.009864\nEpoch: 1874 \tTraining Loss: 0.370588 \tValidation Loss: 1.033144\nEpoch: 1875 \tTraining Loss: 0.375384 \tValidation Loss: 1.128588\nEpoch: 1876 \tTraining Loss: 0.361578 \tValidation Loss: 1.141046\nEpoch: 1877 \tTraining Loss: 0.394399 \tValidation Loss: 0.948851\nEpoch: 1878 \tTraining Loss: 0.398167 \tValidation Loss: 1.062308\nEpoch: 1879 \tTraining Loss: 0.373463 \tValidation Loss: 1.151616\nEpoch: 1880 \tTraining Loss: 0.380984 \tValidation Loss: 1.091656\nEpoch: 1881 \tTraining Loss: 0.366568 \tValidation Loss: 0.970625\nEpoch: 1882 \tTraining Loss: 0.395904 \tValidation Loss: 1.006978\nEpoch: 1883 \tTraining Loss: 0.417591 \tValidation Loss: 0.981882\nEpoch: 1884 \tTraining Loss: 0.361929 \tValidation Loss: 1.036978\nEpoch: 1885 \tTraining Loss: 0.388998 \tValidation Loss: 1.199545\nEpoch: 1886 \tTraining Loss: 0.386823 \tValidation Loss: 1.018115\nEpoch: 1887 \tTraining Loss: 0.405220 \tValidation Loss: 1.037837\nEpoch: 1888 \tTraining Loss: 0.386875 \tValidation Loss: 1.001020\nEpoch: 1889 \tTraining Loss: 0.407785 \tValidation Loss: 1.110580\nEpoch: 1890 \tTraining Loss: 0.386511 \tValidation Loss: 1.289578\nEpoch: 1891 \tTraining Loss: 0.402129 \tValidation Loss: 1.020559\nEpoch: 1892 \tTraining Loss: 0.372438 \tValidation Loss: 1.063951\nEpoch: 1893 \tTraining Loss: 0.413947 \tValidation Loss: 0.947151\nEpoch: 1894 \tTraining Loss: 0.378719 \tValidation Loss: 1.097848\nEpoch: 1895 \tTraining Loss: 0.386791 \tValidation Loss: 1.018722\nEpoch: 1896 \tTraining Loss: 0.385523 \tValidation Loss: 1.195332\nEpoch: 1897 \tTraining Loss: 0.398653 \tValidation Loss: 1.041627\nEpoch: 1898 \tTraining Loss: 0.410128 \tValidation Loss: 1.156983\nEpoch: 1899 \tTraining Loss: 0.407281 \tValidation Loss: 0.998490\nEpoch: 1900 \tTraining Loss: 0.417602 \tValidation Loss: 1.149231\nEpoch: 1901 \tTraining Loss: 0.404341 \tValidation Loss: 0.972153\nEpoch: 1902 \tTraining Loss: 0.377948 \tValidation Loss: 1.149160\nEpoch: 1903 \tTraining Loss: 0.378813 \tValidation Loss: 1.063913\nEpoch: 1904 \tTraining Loss: 0.376793 \tValidation Loss: 1.041621\nEpoch: 1905 \tTraining Loss: 0.381157 \tValidation Loss: 1.153526\nEpoch: 1906 \tTraining Loss: 0.407345 \tValidation Loss: 0.986327\nEpoch: 1907 \tTraining Loss: 0.421184 \tValidation Loss: 1.121508\nEpoch: 1908 \tTraining Loss: 0.405205 \tValidation Loss: 1.052385\nEpoch: 1909 \tTraining Loss: 0.388283 \tValidation Loss: 0.999501\nEpoch: 1910 \tTraining Loss: 0.383897 \tValidation Loss: 1.159726\nEpoch: 1911 \tTraining Loss: 0.384744 \tValidation Loss: 1.015820\nEpoch: 1912 \tTraining Loss: 0.419277 \tValidation Loss: 0.950292\nEpoch: 1913 \tTraining Loss: 0.405672 \tValidation Loss: 1.104967\nEpoch: 1914 \tTraining Loss: 0.388170 \tValidation Loss: 1.036861\nEpoch: 1915 \tTraining Loss: 0.385314 \tValidation Loss: 1.067275\nEpoch: 1916 \tTraining Loss: 0.404526 \tValidation Loss: 1.156009\nEpoch: 1917 \tTraining Loss: 0.405632 \tValidation Loss: 1.077717\nEpoch: 1918 \tTraining Loss: 0.414344 \tValidation Loss: 1.115026\nEpoch: 1919 \tTraining Loss: 0.403148 \tValidation Loss: 1.172978\nEpoch: 1920 \tTraining Loss: 0.369037 \tValidation Loss: 1.015274\nEpoch: 1921 \tTraining Loss: 0.382311 \tValidation Loss: 0.982958\nEpoch: 1922 \tTraining Loss: 0.367508 \tValidation Loss: 1.115689\nEpoch: 1923 \tTraining Loss: 0.346489 \tValidation Loss: 1.033537\nEpoch: 1924 \tTraining Loss: 0.387954 \tValidation Loss: 1.090840\nEpoch: 1925 \tTraining Loss: 0.357390 \tValidation Loss: 1.091370\nEpoch: 1926 \tTraining Loss: 0.348160 \tValidation Loss: 1.026438\nEpoch: 1927 \tTraining Loss: 0.362229 \tValidation Loss: 1.184402\nEpoch: 1928 \tTraining Loss: 0.371792 \tValidation Loss: 1.104071\nEpoch: 1929 \tTraining Loss: 0.378027 \tValidation Loss: 1.112893\nEpoch: 1930 \tTraining Loss: 0.364486 \tValidation Loss: 1.216792\nEpoch: 1931 \tTraining Loss: 0.389616 \tValidation Loss: 1.137775\nEpoch: 1932 \tTraining Loss: 0.376710 \tValidation Loss: 1.109241\nEpoch: 1933 \tTraining Loss: 0.358787 \tValidation Loss: 0.969767\nEpoch: 1934 \tTraining Loss: 0.378659 \tValidation Loss: 1.076125\nEpoch: 1935 \tTraining Loss: 0.377056 \tValidation Loss: 1.021934\nEpoch: 1936 \tTraining Loss: 0.404824 \tValidation Loss: 1.077686\nEpoch: 1937 \tTraining Loss: 0.370837 \tValidation Loss: 1.049709\nEpoch: 1938 \tTraining Loss: 0.377690 \tValidation Loss: 1.061360\nEpoch: 1939 \tTraining Loss: 0.396257 \tValidation Loss: 1.064035\nEpoch: 1940 \tTraining Loss: 0.358316 \tValidation Loss: 0.977088\nEpoch: 1941 \tTraining Loss: 0.408024 \tValidation Loss: 1.083261\nEpoch: 1942 \tTraining Loss: 0.409028 \tValidation Loss: 0.917983\nEpoch: 1943 \tTraining Loss: 0.379532 \tValidation Loss: 1.066190\nEpoch: 1944 \tTraining Loss: 0.365998 \tValidation Loss: 0.992433\nEpoch: 1945 \tTraining Loss: 0.369501 \tValidation Loss: 1.224223\nEpoch: 1946 \tTraining Loss: 0.389269 \tValidation Loss: 0.929327\nEpoch: 1947 \tTraining Loss: 0.369834 \tValidation Loss: 1.051344\nEpoch: 1948 \tTraining Loss: 0.332126 \tValidation Loss: 1.061092\nEpoch: 1949 \tTraining Loss: 0.395239 \tValidation Loss: 1.077778\nEpoch: 1950 \tTraining Loss: 0.432349 \tValidation Loss: 1.087116\nEpoch: 1951 \tTraining Loss: 0.372003 \tValidation Loss: 1.102955\nEpoch: 1952 \tTraining Loss: 0.364398 \tValidation Loss: 1.002707\nEpoch: 1953 \tTraining Loss: 0.402732 \tValidation Loss: 1.024506\nEpoch: 1954 \tTraining Loss: 0.381082 \tValidation Loss: 1.106592\nEpoch: 1955 \tTraining Loss: 0.408561 \tValidation Loss: 1.063127\nEpoch: 1956 \tTraining Loss: 0.425495 \tValidation Loss: 0.997305\nEpoch: 1957 \tTraining Loss: 0.375367 \tValidation Loss: 1.037407\nEpoch: 1958 \tTraining Loss: 0.412698 \tValidation Loss: 1.198117\nEpoch: 1959 \tTraining Loss: 0.394395 \tValidation Loss: 1.022367\nEpoch: 1960 \tTraining Loss: 0.412343 \tValidation Loss: 1.145276\nEpoch: 1961 \tTraining Loss: 0.390767 \tValidation Loss: 1.064845\nEpoch: 1962 \tTraining Loss: 0.372776 \tValidation Loss: 0.993134\nEpoch: 1963 \tTraining Loss: 0.356291 \tValidation Loss: 1.163052\nEpoch: 1964 \tTraining Loss: 0.411305 \tValidation Loss: 1.015947\nEpoch: 1965 \tTraining Loss: 0.374319 \tValidation Loss: 1.214237\nEpoch: 1966 \tTraining Loss: 0.397972 \tValidation Loss: 0.995393\nEpoch: 1967 \tTraining Loss: 0.395047 \tValidation Loss: 1.091482\nEpoch: 1968 \tTraining Loss: 0.390643 \tValidation Loss: 0.979580\nEpoch: 1969 \tTraining Loss: 0.373034 \tValidation Loss: 1.062980\nEpoch: 1970 \tTraining Loss: 0.407768 \tValidation Loss: 1.109758\nEpoch: 1971 \tTraining Loss: 0.388588 \tValidation Loss: 1.136128\nEpoch: 1972 \tTraining Loss: 0.424382 \tValidation Loss: 1.022680\nEpoch: 1973 \tTraining Loss: 0.424532 \tValidation Loss: 1.151955\nEpoch: 1974 \tTraining Loss: 0.365352 \tValidation Loss: 1.136435\nEpoch: 1975 \tTraining Loss: 0.399057 \tValidation Loss: 0.975978\nEpoch: 1976 \tTraining Loss: 0.394666 \tValidation Loss: 0.979394\nEpoch: 1977 \tTraining Loss: 0.381590 \tValidation Loss: 1.012955\nEpoch: 1978 \tTraining Loss: 0.356185 \tValidation Loss: 1.072963\nEpoch: 1979 \tTraining Loss: 0.385075 \tValidation Loss: 1.103950\nEpoch: 1980 \tTraining Loss: 0.408828 \tValidation Loss: 1.035845\nEpoch: 1981 \tTraining Loss: 0.374798 \tValidation Loss: 1.018516\nEpoch: 1982 \tTraining Loss: 0.355488 \tValidation Loss: 1.035150\nEpoch: 1983 \tTraining Loss: 0.380252 \tValidation Loss: 1.086564\nEpoch: 1984 \tTraining Loss: 0.411601 \tValidation Loss: 1.065748\nEpoch: 1985 \tTraining Loss: 0.380327 \tValidation Loss: 1.074095\nEpoch: 1986 \tTraining Loss: 0.395712 \tValidation Loss: 1.029760\nEpoch: 1987 \tTraining Loss: 0.380787 \tValidation Loss: 1.050157\nEpoch: 1988 \tTraining Loss: 0.390725 \tValidation Loss: 0.976960\nEpoch: 1989 \tTraining Loss: 0.387496 \tValidation Loss: 1.114882\nEpoch: 1990 \tTraining Loss: 0.384180 \tValidation Loss: 1.067290\nEpoch: 1991 \tTraining Loss: 0.394517 \tValidation Loss: 0.996129\nEpoch: 1992 \tTraining Loss: 0.393836 \tValidation Loss: 1.068673\nEpoch: 1993 \tTraining Loss: 0.426134 \tValidation Loss: 1.043933\nEpoch: 1994 \tTraining Loss: 0.387555 \tValidation Loss: 1.090923\nEpoch: 1995 \tTraining Loss: 0.371445 \tValidation Loss: 1.016984\nEpoch: 1996 \tTraining Loss: 0.372182 \tValidation Loss: 0.951855\nEpoch: 1997 \tTraining Loss: 0.367815 \tValidation Loss: 1.187413\nEpoch: 1998 \tTraining Loss: 0.424350 \tValidation Loss: 0.934616\nEpoch: 1999 \tTraining Loss: 0.386009 \tValidation Loss: 0.983388\nEpoch: 2000 \tTraining Loss: 0.361660 \tValidation Loss: 1.043069\nTotal time: 2092.206, average time per epoch: 1.046\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 2000\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        #print(target.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    writer.add_scalar('Train/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Valid/Loss', loss, epoch)\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_EEG.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "time_total = time.time() - t0\n",
    "print('Total time: {:4.3f}, average time per epoch: {:4.3f}'.format(time_total, time_total / n_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_EEG.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test Loss: 0.794116\n\nTest Accuracy of   769: 75% (84/111)\nTest Accuracy of   770: 76% (97/127)\nTest Accuracy of   771: 57% (55/96)\nTest Accuracy of   772: 64% (70/109)\n\nTest Accuracy (Overall): 69% (306/443)\n"
    }
   ],
   "source": [
    "# specify the target classes\n",
    "classes = [769, 770, 771, 772]\n",
    "\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    #print(pred)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(correct.shape[0]):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(4):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}