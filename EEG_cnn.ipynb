{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0 1 2 3]\n[0 1 2 3]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "\n",
    "print(np.unique(y_train_valid))\n",
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training/Valid data shape: (2115, 22, 1000)\nTest data shape: (443, 22, 1000)\nTraining/Valid target shape: (2115,)\nTest target shape: (443,)\nPerson train/valid shape: (2115, 1)\nPerson test shape: (443, 1)\n"
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "X_train_valid = torch.from_numpy(X_train_valid).float()\n",
    "y_train_valid = torch.from_numpy(y_train_valid).long()\n",
    "train_data = data.TensorDataset(X_train_valid, y_train_valid)\n",
    "\n",
    "person_train_valid = torch.from_numpy(person_train_valid)\n",
    "person_test = torch.from_numpy(person_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "batch_size = 128\n",
    "valid_size = 0.2\n",
    "\n",
    "\"\"\"\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\"\"\"\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet (modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cuda:0\nTraining on GPU ...\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
    "print(device)\n",
    "if not train_on_gpu:\n",
    "    print('Training on CPU ...')\n",
    "else:\n",
    "    print('Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(22, 64, kernel_size=22, stride=2),\n",
    "            nn.BatchNorm1d(64, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=12, stride=2),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(64, 192, kernel_size=12),\n",
    "            nn.BatchNorm1d(192, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(192, 384, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm1d(384, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(384, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm1d(256, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.Conv1d(256, 256, kernel_size=4),\n",
    "            nn.BatchNorm1d(256, affine=False),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=1),\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm1d(256 * 21),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256 * 21),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256 * 21, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = AlexNet()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.0005, lr=1e-3)\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 1 \tTraining Loss: 0.579764 \tValidation Loss: 0.657254\nValidation loss decreased (0.784589 --> 0.657254).  Saving model ...\nEpoch: 2 \tTraining Loss: 0.537171 \tValidation Loss: 0.635720\nValidation loss decreased (0.657254 --> 0.635720).  Saving model ...\nEpoch: 3 \tTraining Loss: 0.521937 \tValidation Loss: 0.595290\nValidation loss decreased (0.635720 --> 0.595290).  Saving model ...\nEpoch: 4 \tTraining Loss: 0.515310 \tValidation Loss: 0.620110\nEpoch: 5 \tTraining Loss: 0.494190 \tValidation Loss: 0.680836\nEpoch: 6 \tTraining Loss: 0.480008 \tValidation Loss: 0.611136\nEpoch: 7 \tTraining Loss: 0.507477 \tValidation Loss: 0.649809\nEpoch: 8 \tTraining Loss: 0.481546 \tValidation Loss: 0.735551\nEpoch: 9 \tTraining Loss: 0.506273 \tValidation Loss: 0.690732\nEpoch: 10 \tTraining Loss: 0.481854 \tValidation Loss: 0.625786\nEpoch: 11 \tTraining Loss: 0.474032 \tValidation Loss: 0.727183\nEpoch: 12 \tTraining Loss: 0.478123 \tValidation Loss: 0.663439\nEpoch: 13 \tTraining Loss: 0.513632 \tValidation Loss: 0.639207\nEpoch: 14 \tTraining Loss: 0.455975 \tValidation Loss: 0.665159\nEpoch: 15 \tTraining Loss: 0.478691 \tValidation Loss: 0.670272\nEpoch: 16 \tTraining Loss: 0.459982 \tValidation Loss: 0.726378\nEpoch: 17 \tTraining Loss: 0.487988 \tValidation Loss: 0.698620\nEpoch: 18 \tTraining Loss: 0.484069 \tValidation Loss: 0.692356\nEpoch: 19 \tTraining Loss: 0.483024 \tValidation Loss: 0.758603\nEpoch: 20 \tTraining Loss: 0.465998 \tValidation Loss: 0.696206\nEpoch: 21 \tTraining Loss: 0.485979 \tValidation Loss: 0.807054\nEpoch: 22 \tTraining Loss: 0.500847 \tValidation Loss: 0.678874\nEpoch: 23 \tTraining Loss: 0.479011 \tValidation Loss: 0.673213\nEpoch: 24 \tTraining Loss: 0.459280 \tValidation Loss: 0.672990\nEpoch: 25 \tTraining Loss: 0.429494 \tValidation Loss: 0.722532\nEpoch: 26 \tTraining Loss: 0.463298 \tValidation Loss: 0.776496\nEpoch: 27 \tTraining Loss: 0.506613 \tValidation Loss: 0.669589\nEpoch: 28 \tTraining Loss: 0.481199 \tValidation Loss: 0.774172\nEpoch: 29 \tTraining Loss: 0.485359 \tValidation Loss: 0.703183\nEpoch: 30 \tTraining Loss: 0.486478 \tValidation Loss: 0.746648\nEpoch: 31 \tTraining Loss: 0.487275 \tValidation Loss: 0.800896\nEpoch: 32 \tTraining Loss: 0.476612 \tValidation Loss: 0.803515\nEpoch: 33 \tTraining Loss: 0.432905 \tValidation Loss: 0.713107\nEpoch: 34 \tTraining Loss: 0.430055 \tValidation Loss: 0.684936\nEpoch: 35 \tTraining Loss: 0.439562 \tValidation Loss: 0.782225\nEpoch: 36 \tTraining Loss: 0.488077 \tValidation Loss: 0.678968\nEpoch: 37 \tTraining Loss: 0.464784 \tValidation Loss: 0.743958\nEpoch: 38 \tTraining Loss: 0.478315 \tValidation Loss: 0.736082\nEpoch: 39 \tTraining Loss: 0.502375 \tValidation Loss: 0.746133\nEpoch: 40 \tTraining Loss: 0.455229 \tValidation Loss: 0.709976\nEpoch: 41 \tTraining Loss: 0.500423 \tValidation Loss: 0.764079\nEpoch: 42 \tTraining Loss: 0.446806 \tValidation Loss: 0.726654\nEpoch: 43 \tTraining Loss: 0.451550 \tValidation Loss: 0.726448\nEpoch: 44 \tTraining Loss: 0.450864 \tValidation Loss: 0.710991\nEpoch: 45 \tTraining Loss: 0.488391 \tValidation Loss: 0.747602\nEpoch: 46 \tTraining Loss: 0.449287 \tValidation Loss: 0.723042\nEpoch: 47 \tTraining Loss: 0.474174 \tValidation Loss: 0.714993\nEpoch: 48 \tTraining Loss: 0.445400 \tValidation Loss: 0.726945\nEpoch: 49 \tTraining Loss: 0.447918 \tValidation Loss: 0.783083\nEpoch: 50 \tTraining Loss: 0.454053 \tValidation Loss: 0.809993\nEpoch: 51 \tTraining Loss: 0.453314 \tValidation Loss: 0.734297\nEpoch: 52 \tTraining Loss: 0.459475 \tValidation Loss: 0.740868\nEpoch: 53 \tTraining Loss: 0.455617 \tValidation Loss: 0.709061\nEpoch: 54 \tTraining Loss: 0.473527 \tValidation Loss: 0.731628\nEpoch: 55 \tTraining Loss: 0.468057 \tValidation Loss: 0.681530\nEpoch: 56 \tTraining Loss: 0.497102 \tValidation Loss: 0.733099\nEpoch: 57 \tTraining Loss: 0.473060 \tValidation Loss: 0.707631\nEpoch: 58 \tTraining Loss: 0.463098 \tValidation Loss: 0.709787\nEpoch: 59 \tTraining Loss: 0.446812 \tValidation Loss: 0.752453\nEpoch: 60 \tTraining Loss: 0.467771 \tValidation Loss: 0.716507\nEpoch: 61 \tTraining Loss: 0.443552 \tValidation Loss: 0.771531\nEpoch: 62 \tTraining Loss: 0.431818 \tValidation Loss: 0.703769\nEpoch: 63 \tTraining Loss: 0.445114 \tValidation Loss: 0.685948\nEpoch: 64 \tTraining Loss: 0.459622 \tValidation Loss: 0.754918\nEpoch: 65 \tTraining Loss: 0.410927 \tValidation Loss: 0.675684\nEpoch: 66 \tTraining Loss: 0.459229 \tValidation Loss: 0.707511\nEpoch: 67 \tTraining Loss: 0.458427 \tValidation Loss: 0.673375\nEpoch: 68 \tTraining Loss: 0.454894 \tValidation Loss: 0.748759\nEpoch: 69 \tTraining Loss: 0.427533 \tValidation Loss: 0.748393\nEpoch: 70 \tTraining Loss: 0.474377 \tValidation Loss: 0.716741\nEpoch: 71 \tTraining Loss: 0.458985 \tValidation Loss: 0.699658\nEpoch: 72 \tTraining Loss: 0.443914 \tValidation Loss: 0.809674\nEpoch: 73 \tTraining Loss: 0.434000 \tValidation Loss: 0.703960\nEpoch: 74 \tTraining Loss: 0.430131 \tValidation Loss: 0.740366\nEpoch: 75 \tTraining Loss: 0.443724 \tValidation Loss: 0.738864\nEpoch: 76 \tTraining Loss: 0.396499 \tValidation Loss: 0.733946\nEpoch: 77 \tTraining Loss: 0.481937 \tValidation Loss: 0.751725\nEpoch: 78 \tTraining Loss: 0.412547 \tValidation Loss: 0.658562\nEpoch: 79 \tTraining Loss: 0.427608 \tValidation Loss: 0.818914\nEpoch: 80 \tTraining Loss: 0.444039 \tValidation Loss: 0.694065\nEpoch: 81 \tTraining Loss: 0.417804 \tValidation Loss: 0.692119\nEpoch: 82 \tTraining Loss: 0.460816 \tValidation Loss: 0.691763\nEpoch: 83 \tTraining Loss: 0.414390 \tValidation Loss: 0.744437\nEpoch: 84 \tTraining Loss: 0.466396 \tValidation Loss: 0.669836\nEpoch: 85 \tTraining Loss: 0.434847 \tValidation Loss: 0.726413\nEpoch: 86 \tTraining Loss: 0.436171 \tValidation Loss: 0.722724\nEpoch: 87 \tTraining Loss: 0.395606 \tValidation Loss: 0.749163\nEpoch: 88 \tTraining Loss: 0.441502 \tValidation Loss: 0.726641\nEpoch: 89 \tTraining Loss: 0.463980 \tValidation Loss: 0.756207\nEpoch: 90 \tTraining Loss: 0.465462 \tValidation Loss: 0.717801\nEpoch: 91 \tTraining Loss: 0.432239 \tValidation Loss: 0.660797\nEpoch: 92 \tTraining Loss: 0.452243 \tValidation Loss: 0.636951\nEpoch: 93 \tTraining Loss: 0.451516 \tValidation Loss: 0.678284\nEpoch: 94 \tTraining Loss: 0.438015 \tValidation Loss: 0.695217\nEpoch: 95 \tTraining Loss: 0.445914 \tValidation Loss: 0.741368\nEpoch: 96 \tTraining Loss: 0.474736 \tValidation Loss: 0.713291\nEpoch: 97 \tTraining Loss: 0.480708 \tValidation Loss: 0.714705\nEpoch: 98 \tTraining Loss: 0.444370 \tValidation Loss: 0.654948\nEpoch: 99 \tTraining Loss: 0.472741 \tValidation Loss: 0.679947\nEpoch: 100 \tTraining Loss: 0.474419 \tValidation Loss: 0.691157\nEpoch: 101 \tTraining Loss: 0.468347 \tValidation Loss: 0.688071\nEpoch: 102 \tTraining Loss: 0.464787 \tValidation Loss: 0.726001\nEpoch: 103 \tTraining Loss: 0.436887 \tValidation Loss: 0.724181\nEpoch: 104 \tTraining Loss: 0.452552 \tValidation Loss: 0.694105\nEpoch: 105 \tTraining Loss: 0.443936 \tValidation Loss: 0.736295\nEpoch: 106 \tTraining Loss: 0.450762 \tValidation Loss: 0.712229\nEpoch: 107 \tTraining Loss: 0.443272 \tValidation Loss: 0.820917\nEpoch: 108 \tTraining Loss: 0.426252 \tValidation Loss: 0.676282\nEpoch: 109 \tTraining Loss: 0.411919 \tValidation Loss: 0.763168\nEpoch: 110 \tTraining Loss: 0.438912 \tValidation Loss: 0.690759\nEpoch: 111 \tTraining Loss: 0.443786 \tValidation Loss: 0.777695\nEpoch: 112 \tTraining Loss: 0.437929 \tValidation Loss: 0.747880\nEpoch: 113 \tTraining Loss: 0.440351 \tValidation Loss: 0.659241\nEpoch: 114 \tTraining Loss: 0.441004 \tValidation Loss: 0.681386\nEpoch: 115 \tTraining Loss: 0.446583 \tValidation Loss: 0.817912\nEpoch: 116 \tTraining Loss: 0.478931 \tValidation Loss: 0.832648\nEpoch: 117 \tTraining Loss: 0.476292 \tValidation Loss: 0.743626\nEpoch: 118 \tTraining Loss: 0.478241 \tValidation Loss: 0.743536\nEpoch: 119 \tTraining Loss: 0.459722 \tValidation Loss: 0.746882\nEpoch: 120 \tTraining Loss: 0.454485 \tValidation Loss: 0.711457\nEpoch: 121 \tTraining Loss: 0.436643 \tValidation Loss: 0.717481\nEpoch: 122 \tTraining Loss: 0.453097 \tValidation Loss: 0.717173\nEpoch: 123 \tTraining Loss: 0.419972 \tValidation Loss: 0.782051\nEpoch: 124 \tTraining Loss: 0.438742 \tValidation Loss: 0.678220\nEpoch: 125 \tTraining Loss: 0.448683 \tValidation Loss: 0.758930\nEpoch: 126 \tTraining Loss: 0.491893 \tValidation Loss: 0.758097\nEpoch: 127 \tTraining Loss: 0.437822 \tValidation Loss: 0.686520\nEpoch: 128 \tTraining Loss: 0.449684 \tValidation Loss: 0.751670\nEpoch: 129 \tTraining Loss: 0.441657 \tValidation Loss: 0.692231\nEpoch: 130 \tTraining Loss: 0.448769 \tValidation Loss: 0.790204\nEpoch: 131 \tTraining Loss: 0.442444 \tValidation Loss: 0.701574\nEpoch: 132 \tTraining Loss: 0.454168 \tValidation Loss: 0.690602\nEpoch: 133 \tTraining Loss: 0.463647 \tValidation Loss: 0.714449\nEpoch: 134 \tTraining Loss: 0.453568 \tValidation Loss: 0.723103\nEpoch: 135 \tTraining Loss: 0.434160 \tValidation Loss: 0.774365\nEpoch: 136 \tTraining Loss: 0.477413 \tValidation Loss: 0.725454\nEpoch: 137 \tTraining Loss: 0.442229 \tValidation Loss: 0.761989\nEpoch: 138 \tTraining Loss: 0.406091 \tValidation Loss: 0.792977\nEpoch: 139 \tTraining Loss: 0.393869 \tValidation Loss: 0.692124\nEpoch: 140 \tTraining Loss: 0.456320 \tValidation Loss: 0.705960\nEpoch: 141 \tTraining Loss: 0.420797 \tValidation Loss: 0.709154\nEpoch: 142 \tTraining Loss: 0.428945 \tValidation Loss: 0.699035\nEpoch: 143 \tTraining Loss: 0.425943 \tValidation Loss: 0.737092\nEpoch: 144 \tTraining Loss: 0.437128 \tValidation Loss: 0.781505\nEpoch: 145 \tTraining Loss: 0.438380 \tValidation Loss: 0.760646\nEpoch: 146 \tTraining Loss: 0.428500 \tValidation Loss: 0.846148\nEpoch: 147 \tTraining Loss: 0.458187 \tValidation Loss: 0.721353\nEpoch: 148 \tTraining Loss: 0.446218 \tValidation Loss: 0.859481\nEpoch: 149 \tTraining Loss: 0.419552 \tValidation Loss: 0.719967\nEpoch: 150 \tTraining Loss: 0.442426 \tValidation Loss: 0.765449\nEpoch: 151 \tTraining Loss: 0.417287 \tValidation Loss: 0.735706\nEpoch: 152 \tTraining Loss: 0.423825 \tValidation Loss: 0.749413\nEpoch: 153 \tTraining Loss: 0.426676 \tValidation Loss: 0.750442\nEpoch: 154 \tTraining Loss: 0.418116 \tValidation Loss: 0.706206\nEpoch: 155 \tTraining Loss: 0.432393 \tValidation Loss: 0.720274\nEpoch: 156 \tTraining Loss: 0.442368 \tValidation Loss: 0.730970\nEpoch: 157 \tTraining Loss: 0.408216 \tValidation Loss: 0.822527\nEpoch: 158 \tTraining Loss: 0.464656 \tValidation Loss: 0.670291\nEpoch: 159 \tTraining Loss: 0.408690 \tValidation Loss: 0.776941\nEpoch: 160 \tTraining Loss: 0.410032 \tValidation Loss: 0.674845\nEpoch: 161 \tTraining Loss: 0.424232 \tValidation Loss: 0.785462\nEpoch: 162 \tTraining Loss: 0.447985 \tValidation Loss: 0.676068\nEpoch: 163 \tTraining Loss: 0.431419 \tValidation Loss: 0.704881\nEpoch: 164 \tTraining Loss: 0.449211 \tValidation Loss: 0.752104\nEpoch: 165 \tTraining Loss: 0.452919 \tValidation Loss: 0.742038\nEpoch: 166 \tTraining Loss: 0.423963 \tValidation Loss: 0.686646\nEpoch: 167 \tTraining Loss: 0.417819 \tValidation Loss: 0.669982\nEpoch: 168 \tTraining Loss: 0.413265 \tValidation Loss: 0.706134\nEpoch: 169 \tTraining Loss: 0.407632 \tValidation Loss: 0.735511\nEpoch: 170 \tTraining Loss: 0.420917 \tValidation Loss: 0.876025\nEpoch: 171 \tTraining Loss: 0.415438 \tValidation Loss: 0.688808\nEpoch: 172 \tTraining Loss: 0.413376 \tValidation Loss: 0.705755\nEpoch: 173 \tTraining Loss: 0.380973 \tValidation Loss: 0.789025\nEpoch: 174 \tTraining Loss: 0.434305 \tValidation Loss: 0.711739\nEpoch: 175 \tTraining Loss: 0.393397 \tValidation Loss: 0.752388\nEpoch: 176 \tTraining Loss: 0.403695 \tValidation Loss: 0.665639\nEpoch: 177 \tTraining Loss: 0.436098 \tValidation Loss: 0.718747\nEpoch: 178 \tTraining Loss: 0.437170 \tValidation Loss: 0.716270\nEpoch: 179 \tTraining Loss: 0.418223 \tValidation Loss: 0.765027\nEpoch: 180 \tTraining Loss: 0.436557 \tValidation Loss: 0.873417\nEpoch: 181 \tTraining Loss: 0.448126 \tValidation Loss: 0.691212\nEpoch: 182 \tTraining Loss: 0.433346 \tValidation Loss: 0.771398\nEpoch: 183 \tTraining Loss: 0.439141 \tValidation Loss: 0.822065\nEpoch: 184 \tTraining Loss: 0.432684 \tValidation Loss: 0.803804\nEpoch: 185 \tTraining Loss: 0.411706 \tValidation Loss: 0.756445\nEpoch: 186 \tTraining Loss: 0.454662 \tValidation Loss: 0.703783\nEpoch: 187 \tTraining Loss: 0.462598 \tValidation Loss: 0.757845\nEpoch: 188 \tTraining Loss: 0.411420 \tValidation Loss: 0.788149\nEpoch: 189 \tTraining Loss: 0.405990 \tValidation Loss: 0.761873\nEpoch: 190 \tTraining Loss: 0.414262 \tValidation Loss: 0.779411\nEpoch: 191 \tTraining Loss: 0.398349 \tValidation Loss: 0.686590\nEpoch: 192 \tTraining Loss: 0.410532 \tValidation Loss: 0.812901\nEpoch: 193 \tTraining Loss: 0.432850 \tValidation Loss: 0.769055\nEpoch: 194 \tTraining Loss: 0.405909 \tValidation Loss: 0.779253\nEpoch: 195 \tTraining Loss: 0.380462 \tValidation Loss: 0.718917\nEpoch: 196 \tTraining Loss: 0.392612 \tValidation Loss: 0.728460\nEpoch: 197 \tTraining Loss: 0.419116 \tValidation Loss: 0.731403\nEpoch: 198 \tTraining Loss: 0.421134 \tValidation Loss: 0.725127\nEpoch: 199 \tTraining Loss: 0.432244 \tValidation Loss: 0.763343\nEpoch: 200 \tTraining Loss: 0.415133 \tValidation Loss: 0.680532\nEpoch: 201 \tTraining Loss: 0.433543 \tValidation Loss: 0.781479\nEpoch: 202 \tTraining Loss: 0.419739 \tValidation Loss: 0.767878\nEpoch: 203 \tTraining Loss: 0.414753 \tValidation Loss: 0.737551\nEpoch: 204 \tTraining Loss: 0.441756 \tValidation Loss: 0.674543\nEpoch: 205 \tTraining Loss: 0.423090 \tValidation Loss: 0.740342\nEpoch: 206 \tTraining Loss: 0.393759 \tValidation Loss: 0.695867\nEpoch: 207 \tTraining Loss: 0.377262 \tValidation Loss: 0.777293\nEpoch: 208 \tTraining Loss: 0.419353 \tValidation Loss: 0.841303\nEpoch: 209 \tTraining Loss: 0.417326 \tValidation Loss: 0.737434\nEpoch: 210 \tTraining Loss: 0.410666 \tValidation Loss: 0.746272\nEpoch: 211 \tTraining Loss: 0.390773 \tValidation Loss: 0.703823\nEpoch: 212 \tTraining Loss: 0.419381 \tValidation Loss: 0.679526\nEpoch: 213 \tTraining Loss: 0.426490 \tValidation Loss: 0.857975\nEpoch: 214 \tTraining Loss: 0.438134 \tValidation Loss: 0.717727\nEpoch: 215 \tTraining Loss: 0.418227 \tValidation Loss: 0.826185\nEpoch: 216 \tTraining Loss: 0.432804 \tValidation Loss: 0.775412\nEpoch: 217 \tTraining Loss: 0.413589 \tValidation Loss: 0.720095\nEpoch: 218 \tTraining Loss: 0.404595 \tValidation Loss: 0.783438\nEpoch: 219 \tTraining Loss: 0.419334 \tValidation Loss: 0.723628\nEpoch: 220 \tTraining Loss: 0.407753 \tValidation Loss: 0.753263\nEpoch: 221 \tTraining Loss: 0.430750 \tValidation Loss: 0.800889\nEpoch: 222 \tTraining Loss: 0.390319 \tValidation Loss: 0.820122\nEpoch: 223 \tTraining Loss: 0.387118 \tValidation Loss: 0.732753\nEpoch: 224 \tTraining Loss: 0.386210 \tValidation Loss: 0.906990\nEpoch: 225 \tTraining Loss: 0.418692 \tValidation Loss: 0.734581\nEpoch: 226 \tTraining Loss: 0.467946 \tValidation Loss: 0.771000\nEpoch: 227 \tTraining Loss: 0.372246 \tValidation Loss: 0.748209\nEpoch: 228 \tTraining Loss: 0.437029 \tValidation Loss: 0.763700\nEpoch: 229 \tTraining Loss: 0.416206 \tValidation Loss: 0.689936\nEpoch: 230 \tTraining Loss: 0.432626 \tValidation Loss: 0.688185\nEpoch: 231 \tTraining Loss: 0.431600 \tValidation Loss: 0.748473\nEpoch: 232 \tTraining Loss: 0.403477 \tValidation Loss: 0.753642\nEpoch: 233 \tTraining Loss: 0.411378 \tValidation Loss: 0.689349\nEpoch: 234 \tTraining Loss: 0.416707 \tValidation Loss: 0.758233\nEpoch: 235 \tTraining Loss: 0.425818 \tValidation Loss: 0.747100\nEpoch: 236 \tTraining Loss: 0.440619 \tValidation Loss: 0.730847\nEpoch: 237 \tTraining Loss: 0.413782 \tValidation Loss: 0.740686\nEpoch: 238 \tTraining Loss: 0.387839 \tValidation Loss: 0.721970\nEpoch: 239 \tTraining Loss: 0.413030 \tValidation Loss: 0.816149\nEpoch: 240 \tTraining Loss: 0.376409 \tValidation Loss: 0.734757\nEpoch: 241 \tTraining Loss: 0.449548 \tValidation Loss: 0.715600\nEpoch: 242 \tTraining Loss: 0.443116 \tValidation Loss: 0.826215\nEpoch: 243 \tTraining Loss: 0.398436 \tValidation Loss: 0.842042\nEpoch: 244 \tTraining Loss: 0.406812 \tValidation Loss: 0.799330\nEpoch: 245 \tTraining Loss: 0.400198 \tValidation Loss: 0.742889\nEpoch: 246 \tTraining Loss: 0.419339 \tValidation Loss: 0.845668\nEpoch: 247 \tTraining Loss: 0.401800 \tValidation Loss: 0.779169\nEpoch: 248 \tTraining Loss: 0.418934 \tValidation Loss: 0.916695\nEpoch: 249 \tTraining Loss: 0.410833 \tValidation Loss: 0.678856\nEpoch: 250 \tTraining Loss: 0.442491 \tValidation Loss: 0.826061\nEpoch: 251 \tTraining Loss: 0.421817 \tValidation Loss: 0.761289\nEpoch: 252 \tTraining Loss: 0.415407 \tValidation Loss: 0.746156\nEpoch: 253 \tTraining Loss: 0.407692 \tValidation Loss: 0.784309\nEpoch: 254 \tTraining Loss: 0.408972 \tValidation Loss: 0.735966\nEpoch: 255 \tTraining Loss: 0.418141 \tValidation Loss: 0.691625\nEpoch: 256 \tTraining Loss: 0.411548 \tValidation Loss: 0.723384\nEpoch: 257 \tTraining Loss: 0.400287 \tValidation Loss: 0.737104\nEpoch: 258 \tTraining Loss: 0.410627 \tValidation Loss: 0.760019\nEpoch: 259 \tTraining Loss: 0.401305 \tValidation Loss: 0.779615\nEpoch: 260 \tTraining Loss: 0.407879 \tValidation Loss: 0.750486\nEpoch: 261 \tTraining Loss: 0.419641 \tValidation Loss: 0.757660\nEpoch: 262 \tTraining Loss: 0.421387 \tValidation Loss: 0.738395\nEpoch: 263 \tTraining Loss: 0.427795 \tValidation Loss: 0.792992\nEpoch: 264 \tTraining Loss: 0.429360 \tValidation Loss: 0.815120\nEpoch: 265 \tTraining Loss: 0.419407 \tValidation Loss: 0.794210\nEpoch: 266 \tTraining Loss: 0.414505 \tValidation Loss: 0.805317\nEpoch: 267 \tTraining Loss: 0.433636 \tValidation Loss: 0.694416\nEpoch: 268 \tTraining Loss: 0.396502 \tValidation Loss: 0.762667\nEpoch: 269 \tTraining Loss: 0.392100 \tValidation Loss: 0.773293\nEpoch: 270 \tTraining Loss: 0.398628 \tValidation Loss: 0.752613\nEpoch: 271 \tTraining Loss: 0.424368 \tValidation Loss: 0.782610\nEpoch: 272 \tTraining Loss: 0.410924 \tValidation Loss: 0.688636\nEpoch: 273 \tTraining Loss: 0.461135 \tValidation Loss: 0.787074\nEpoch: 274 \tTraining Loss: 0.428003 \tValidation Loss: 0.734059\nEpoch: 275 \tTraining Loss: 0.448540 \tValidation Loss: 0.705512\nEpoch: 276 \tTraining Loss: 0.439181 \tValidation Loss: 0.770659\nEpoch: 277 \tTraining Loss: 0.408309 \tValidation Loss: 0.898248\nEpoch: 278 \tTraining Loss: 0.410358 \tValidation Loss: 0.742923\nEpoch: 279 \tTraining Loss: 0.397159 \tValidation Loss: 0.828944\nEpoch: 280 \tTraining Loss: 0.426709 \tValidation Loss: 0.794667\nEpoch: 281 \tTraining Loss: 0.379008 \tValidation Loss: 0.704373\nEpoch: 282 \tTraining Loss: 0.389939 \tValidation Loss: 0.797731\nEpoch: 283 \tTraining Loss: 0.415288 \tValidation Loss: 0.764623\nEpoch: 284 \tTraining Loss: 0.403268 \tValidation Loss: 0.746509\nEpoch: 285 \tTraining Loss: 0.408391 \tValidation Loss: 0.748140\nEpoch: 286 \tTraining Loss: 0.390863 \tValidation Loss: 0.812651\nEpoch: 287 \tTraining Loss: 0.407898 \tValidation Loss: 0.765654\nEpoch: 288 \tTraining Loss: 0.408532 \tValidation Loss: 0.763399\nEpoch: 289 \tTraining Loss: 0.411205 \tValidation Loss: 0.831190\nEpoch: 290 \tTraining Loss: 0.434789 \tValidation Loss: 0.772343\nEpoch: 291 \tTraining Loss: 0.412870 \tValidation Loss: 0.835344\nEpoch: 292 \tTraining Loss: 0.403610 \tValidation Loss: 0.775136\nEpoch: 293 \tTraining Loss: 0.393554 \tValidation Loss: 0.729953\nEpoch: 294 \tTraining Loss: 0.434907 \tValidation Loss: 0.762597\nEpoch: 295 \tTraining Loss: 0.382249 \tValidation Loss: 0.840361\nEpoch: 296 \tTraining Loss: 0.413001 \tValidation Loss: 0.694386\nEpoch: 297 \tTraining Loss: 0.427636 \tValidation Loss: 0.720563\nEpoch: 298 \tTraining Loss: 0.425572 \tValidation Loss: 0.811579\nEpoch: 299 \tTraining Loss: 0.411713 \tValidation Loss: 0.796709\nEpoch: 300 \tTraining Loss: 0.421961 \tValidation Loss: 0.775662\n"
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 300\n",
    "\n",
    "valid_loss_min = 0.784589\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        #print(target.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_EEG.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_EEG_67%.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "test_data = data.TensorDataset(X_test, y_test)\n",
    "\n",
    "test_loader = data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test Loss: 0.852765\n\nTest Accuracy of   769: 72% (80/111)\nTest Accuracy of   770: 79% (101/127)\nTest Accuracy of   771: 57% (55/96)\nTest Accuracy of   772: 59% (65/109)\n\nTest Accuracy (Overall): 67% (301/443)\n"
    }
   ],
   "source": [
    "# specify the target classes\n",
    "classes = [769, 770, 771, 772]\n",
    "\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    #print(pred)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(correct.shape[0]):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(4):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}